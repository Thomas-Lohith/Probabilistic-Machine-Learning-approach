{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /data/pool/c8x-98x/pml/venv/bin/pip3: /home/c8x-98x/Probabilistic-Machine-Learning-approach/venv/bin/python: bad interpreter: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "\n",
    "! pip3 install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        day                                    hour_file  \\\n",
      "0  20241221  M001_2024-12-21_00-00-00_gg-34_int-1_th.csv   \n",
      "1  20241221  M001_2024-12-21_00-00-00_gg-34_int-1_th.csv   \n",
      "\n",
      "                start_time                 end_time      mean      variance  \\\n",
      "0  2024/12/21 00:00:00:000  2024/12/21 00:00:00:990  1.006137  4.607076e-08   \n",
      "1  2024/12/21 00:00:01:000  2024/12/21 00:00:01:990       NaN           NaN   \n",
      "\n",
      "   log_variance  \n",
      "0    -16.893087  \n",
      "1           NaN  \n",
      "500\n",
      "2024/12/21 00:16:40:000\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1500",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/data/pool/c8x-98x/pml/venv/lib/python3.10/site-packages/pandas/core/indexes/range.py:413\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 413\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_range\u001b[39m.\u001b[39;49mindex(new_key)\n\u001b[1;32m    414\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "\u001b[0;31mValueError\u001b[0m: 1500 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(df))\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(df[\u001b[39m'\u001b[39m\u001b[39mstart_time\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m1000\u001b[39m])\n\u001b[0;32m---> 11\u001b[0m \u001b[39mprint\u001b[39m(df[\u001b[39m'\u001b[39;49m\u001b[39mstart_time\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m1500\u001b[39;49m])\n\u001b[1;32m     12\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mplot(figsize\u001b[39m=\u001b[39m(\u001b[39m12\u001b[39m, \u001b[39m6\u001b[39m))\n\u001b[1;32m     13\u001b[0m plt\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[0;32m/data/pool/c8x-98x/pml/venv/lib/python3.10/site-packages/pandas/core/series.py:1130\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[1;32m   1129\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[1;32m   1132\u001b[0m \u001b[39m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m \u001b[39m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m \u001b[39mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m/data/pool/c8x-98x/pml/venv/lib/python3.10/site-packages/pandas/core/series.py:1246\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[1;32m   1245\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1246\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[1;32m   1248\u001b[0m \u001b[39mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1249\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m/data/pool/c8x-98x/pml/venv/lib/python3.10/site-packages/pandas/core/indexes/range.py:415\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_range\u001b[39m.\u001b[39mindex(new_key)\n\u001b[1;32m    414\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 415\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39merr\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Hashable):\n\u001b[1;32m    417\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 1500"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/data/pool/c8x-98x/bridge_data/100_days/20241221.csv', index_col=None)\n",
    "\n",
    "print(df.head(2))\n",
    "df= df.filter(items=['start_time', 'mean', 'log_variance'])\n",
    "#df.to_string(index=False)\n",
    "\n",
    "\n",
    "#df.to_csv('file.csv')\n",
    "print(len(df))\n",
    "print(df['start_time'][1000])\n",
    "print(df['start_time'][1500])\n",
    "\n",
    "df =df[1000:1500]\n",
    "df['mean'].plot(figsize=(12, 6))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "df['log_variance'].plot(figsize=(12, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#script t0 fileter the m,issing data\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def filter_non_missing_csvs(folder_path, subfolder_name=\"no_missing data\"):\n",
    "   \n",
    "    folder = Path(folder_path)\n",
    "    subfolder = folder / subfolder_name\n",
    "    subfolder.mkdir(exist_ok=True)\n",
    "\n",
    "    csv_files = list(folder.glob(\"*.csv\"))\n",
    "    print(f\"Found {len(csv_files)} CSV files\")\n",
    "\n",
    "    non_missing_count = 0\n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            has_missing = df.isnull().any().any()\n",
    "\n",
    "            if not has_missing:\n",
    "                # Write the same data into the subfolder\n",
    "                out_path = subfolder / csv_file.name\n",
    "                df.to_csv(out_path, index=False)\n",
    "                non_missing_count += 1\n",
    "                print(f\" Kept: {csv_file.name}\")\n",
    "            else:\n",
    "                print(f\" Skipped (missing data): {csv_file.name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Skipped (error reading): {csv_file.name} - {str(e)}\")\n",
    "\n",
    "    print(f\"\\nCompleted! {non_missing_count}/{len(csv_files)} files written to '{subfolder_name}'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filter_non_missing_csvs(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_sensor_files(root_dir, sensor_name, output_dir, chunk_size=100):\n",
    "    \"\"\"\n",
    "    Reduce frequency of sensor data by computing mean and variance every 'chunk_size' samples.\n",
    "    Applies log-normal transform on variance.\n",
    "    Saves one reduced CSV per day into the specified output directory.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Loop through each day folder\n",
    "    for day_folder in sorted(os.listdir(root_dir)):\n",
    "        day_path = os.path.join(root_dir, day_folder)\n",
    "        if not os.path.isdir(day_path):\n",
    "            continue\n",
    "\n",
    "        csv_acc_path = os.path.join(day_path, \"csv_acc\")\n",
    "        if not os.path.exists(csv_acc_path):\n",
    "            print(f\"Skipping {day_folder}: no csv_acc folder found\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nProcessing day: {day_folder}\")\n",
    "        results = []\n",
    "\n",
    "        # Process hourly CSVs inside csv_acc folder\n",
    "        for file in tqdm(sorted(os.listdir(csv_acc_path)), desc=f\"{day_folder}\", unit=\"file\"):\n",
    "            if not file.endswith(\".csv\"):\n",
    "                continue\n",
    "\n",
    "            file_path = os.path.join(csv_acc_path, file)\n",
    "\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, usecols=[\"time\", sensor_name], sep=';')\n",
    "            except ValueError:\n",
    "                print(f\"Skipping file (missing columns): {file_path}, check the column names!\")\n",
    "                continue\n",
    "\n",
    "            # Compute reduced features in chunks\n",
    "            for i in range(0, len(df), chunk_size):\n",
    "                chunk = df.iloc[i:i + chunk_size]\n",
    "                if len(chunk) < chunk_size:\n",
    "                    continue  # skip incomplete chunk at end\n",
    "\n",
    "                mean_val = chunk[sensor_name].mean()\n",
    "                var_val = chunk[sensor_name].var()\n",
    "                log_var = log_normal_variance(var_val)\n",
    "\n",
    "                results.append({\n",
    "                    \"day\": day_folder,\n",
    "                    \"hour_file\": file,\n",
    "                    \"start_time\": chunk[\"time\"].iloc[0],\n",
    "                    \"end_time\": chunk[\"time\"].iloc[-1],\n",
    "                    \"mean\": mean_val,\n",
    "                    \"variance\": var_val,\n",
    "                    \"log_variance\": log_var,\n",
    "                })\n",
    "\n",
    "        # Save the day's result to a CSV file\n",
    "        if results:\n",
    "            results_df = pd.DataFrame(results)\n",
    "            output_path = os.path.join(output_dir, f\"{day_folder}.csv\")\n",
    "            results_df.to_csv(output_path, index=False)\n",
    "            print(f\"Saved reduced CSV for {day_folder} â†’ {output_path} ({len(results_df)} rows)\")\n",
    "        else:\n",
    "            print(f\" No valid data found for {day_folder}\")\n",
    "\n",
    "\n",
    "\n",
    "def log_normal_variance(variance):\n",
    "    \"\"\"Apply log-normal transform safely to variance values.\"\"\"\n",
    "    if variance <= 0 or pd.isna(variance):\n",
    "        return np.nan\n",
    "    return np.log(variance)\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Reduce frequency of sensor CSV data (daily outputs)\")\n",
    "    parser.add_argument(\"--root_dir\", type=str, required=True, help=\"Path to parent folder containing date folders\")\n",
    "    parser.add_argument(\"--sensor_channel\", type=str, required=True, help=\"Sensor column name (e.g., 03091002_x)\")\n",
    "    parser.add_argument(\"--chunk_size\", type=int, default=100, help=\"Number of samples per averaging chunk\")\n",
    "    parser.add_argument(\"--output\", type=str, required=True, help=\"Directory to save reduced daily CSVs\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    process_sensor_files(args.root_dir, args.sensor_channel, args.output, args.chunk_size)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     time  03091002_x\n",
      "0 2024-11-26 00:00:00.000    1.022331\n",
      "1 2024-11-26 00:00:00.010    1.022461\n",
      "2 2024-11-26 00:00:00.020    1.022875\n",
      "3 2024-11-26 00:00:00.030    1.022477\n",
      "4 2024-11-26 00:00:00.040    1.022353\n",
      "                           time  03091002_x\n",
      "8639995 2024-11-26 23:59:59.950    1.022468\n",
      "8639996 2024-11-26 23:59:59.960    1.022270\n",
      "8639997 2024-11-26 23:59:59.970    1.022537\n",
      "8639998 2024-11-26 23:59:59.980    1.023083\n",
      "8639999 2024-11-26 23:59:59.990    1.022657\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('/data/pool/c8x-98x/test_merge_script/20241126/20241126.parquet')\n",
    "print(df.head())\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
